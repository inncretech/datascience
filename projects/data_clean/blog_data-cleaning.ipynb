{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"logo.png\">\n",
    " \n",
    " <B><p style=\"text-align:center; color: blue; font-size: 30px \"> Inncretech Project \n",
    " <br><br>\n",
    " <I style= \"text-align:center;color:black; font-size: 20px\"><B style = \"color:red\">Inn</B>ovation <B style = \"color:red\">Cre</B>ativity <B style = \"color:red\">Tech</B>nology: Engineering Imaginations</I>\n",
    " </B></p>\n",
    " \n",
    " <p>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size=\"5\" color=\"brown\">Data Science </font> </b> <p>\n",
    "Data Science has been a growing field of study, research and disciple that supports extensive thinking on various scientific methods, processes and systems to extract knowledge, predict insights and contribute results for making various kind of business, scientific, computational or linguistic results. </p>\n",
    "\n",
    "<b><font size=\"5\" color=\"brown\"> Data Cleaning </font> </b> <p>\n",
    "Data Cleaning is an integral part of data science where data scientists have to spend almost 80% of their time. Data cleaning helps remove abnormality in data and makes it consistent for data sciencetists to understand, run algorithms and make decisions.  Data Cleaning passes through several process of structuring, restructuring, modifications and cleaning.  \n",
    "\n",
    "</p>\n",
    "\n",
    "In this blog, we will discuss basic data cleaning process using <b>[Pandas](https://pandas.pydata.org/pandas-docs/stable/) </b>library and learn about basic things a data science engineers do. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import required library.\n",
    "\n",
    "In this blog, we only used pandas for basic work on data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use basic numpy library functionalities for basic data processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Import the data file\n",
    "\n",
    "\n",
    "Pandas support several kinds of data file types like csv, json, xls, html, fwf, hdf, pickle etc and can be used by many engineers to convert from one format to the other as well. Pandas gives this flexibility to the users to work on other python library by converting different file types to the supported one's. For exmple, if you are using Graphlab library and want to work on xls file type, graphlab may not support, so you need to convert xls file to supported file type like csv to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('file.csv') # you may import other file format instead of csv\n",
    "\n",
    "# df = pd.read_csv('/path/file.csv') # you may mention file path before file name based on its location\n",
    "\n",
    "# df = pd.read_csv('/path/file.csv', encoding = 'utf-8')  # you can replace encoding type as per the need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that you may have to mention file path and encoding type as per the need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Have first look of the data \n",
    "\n",
    "Depending on the type of data, we will see head and tail of data to observe at basic level what all kind of data are present in the file. At this point, you may also use other libraries of python and see visuals from basic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()  # shows the first five rows \n",
    "\n",
    "# df.head(1) # Equivalent to df.head(n=1)\n",
    "\n",
    "# df.tail() # shows the last five rows\n",
    "\n",
    "#df.head(n=0) # gives you just columns but not any rows, n=0 means no of rows to be displayed is zer0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Describe individual columns to see more details\n",
    "\n",
    "Each column can have possible dirty data or duplicated data. With dirty data, I mean missing, unstructured or incomplete data. Any individual column can have many number of inconsistent data that may create problem when analysing and in order to get rid of them, we first need to figure out what kind of inconsistency the data have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.column_name.describe()\n",
    "\n",
    "#df.Business_Name.describe()\n",
    "\n",
    "# df.Address_1.describe() # gives detailed information about "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Describe function can help you analyse the kind of \n",
    "<br>\n",
    "\n",
    "count:                                                   100000\n",
    "<br>\n",
    "\n",
    "unique:                                                84505\n",
    "<br>\n",
    "\n",
    "top:       ---- / ?                                      ...\n",
    "<br>\n",
    "\n",
    "freq:                                                   1607\n",
    "<br>\n",
    "\n",
    "Name: Column_name, dtype: object\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: FInding missing values\n",
    "\n",
    "\n",
    "Most of the dataset have some missing values that are termed as Nan, null,  not availabe or missing values. Term NaN is in numeric arrays, None/NaN in object arrays. We first check if we have missing values and then we find some replacing techniques to go further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give you boolean result, True or False and based on the result, you may decide further operations on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Swapping Missing values with default values\n",
    "\n",
    "Missing values can create several problems in later phase of data engineering if not handled effeciently. So, we fill all the missing values with default values. Dropping rows with missing values at this phase can be dangerous and can be done after this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.column_name = df.column_name.fillna('')\n",
    "\n",
    "# df.column_name = df.column_name.fillna('Missing')   # This will put 'Missing' at spot of all missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, repeat step 6 and see if you still have missing values, hope you get False.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 8: Check data type of each column and change if necessary\n",
    "\n",
    "\n",
    "In step 5, we can observe dtype at the end of description that we get using describe() function and we can see if the data type of each column matches the needed data type, we keep otherwise change. In usual practise, sometimes int/float datatypes are represented as str/objects, in that case, we need to change the data type and convert it into the required format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['column_name'] = df['column_name'].astype(str)\n",
    "\n",
    "#df['column_1'] = df['column_1'].astype(str)\n",
    "\n",
    "#df['column_2'] = df['column_2'].astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get all the columns data type together can be as follows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give you list of all columns and corresponding data types. Please note that in Python 3, str data type is referred as object data type. Repeat Step 5 and see if values changes and keep comparing dataframe with its previous state and new state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Delete Blank Columns \n",
    "\n",
    "For large datasets, having columns with all entries blank or null or no data can be deleted. You first need to see if a particular column is useful or not and then decide this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['column_name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of doing this is with dropna() function where we can define threshold that we will discuss in later steps. We drop the column having all null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1, how=’all’)\n",
    "\n",
    "#df = df.dropna(axis=1, how=’any’)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get rid of all such columns having almost no data and proceed further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Delete Blank Rows\n",
    "\n",
    "Depending on the required threshold, we may delete rows with values less than our expected threshold value. For example, if we have thresh = 5, means, we should have atleast 5 not-null values in the given row. By default, dropna() function deletes all the rows having atleats one null value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna() # Drops all rows with null values\n",
    "\n",
    "#df.dropna(thresh=10)  # Drops all rows with less than 10 values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Working on Strings\n",
    "\n",
    "\n",
    "Columns with string data types can undergo several data processing. Various methods can be applied that can go beyond the scope of this blog, we will try to briefly explain how things work with strings. \n",
    "\n",
    "\n",
    "#### a) Removing white spaces\n",
    "\n",
    "White spaces can potentially create a major issue of disturbance during processing, cleaning and observing data. In string columns, we need to cleanup white spaces on edges i.e trailing whitespace and replace with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['String_column_name'] = df['String_column_name'].str.strip()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Making string cases consistent\n",
    "\n",
    "Keeping data in one case, either upper or lower can have many benifits. Usually we keep all column values in one case to perform efficient string matching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['String_column_name'] = df['String_column_name'].str.lower() # converts all data in the column in lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Introducing binding in strings\n",
    "\n",
    "We replace all the white spaces between words and attach them using underscore `_` to bing all the strings together with no empty space. We may also use some other key to bind data together and work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['String_column_name'] = df['String_column_name'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) String Matching\n",
    "\n",
    "String matching can be performed to deduplicate data. One value in a string column can be present in many different ways. Ex- location of states of US can be represented in a dataset as <b>`New York` </b> and <b> `New York, NY` </b>.  Several algorithms can sort out these problems of string matching like fuzzy matching, similarity scores of column elements etc.  and resulting data processing. We will simply keep this task for readers to try as it goes beyond the scope of this blog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Drop Duplicates \n",
    "\n",
    "In most of datasets, there are chances of duplication and after all the required processing, we can further drop all duplicates and get unique non repeating data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['Dup_column1', 'Dup_column2'], keep=False) # drop all duplicates\n",
    "\n",
    "#df = df.drop_duplicates(subset=['Dup_column1', 'Dup_column2'], keep= 'first') #keeps first occurance of duplicates\n",
    "#df = df.drop_duplicates(subset=['Dup_column1', 'Dup_column2'], keep= 'last') # keeps last occurance of duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Remove Special Characters\n",
    "\n",
    "In string column, we may have garbage values, chinese scripts, special characters and many other abnormal text that can create problems in observing data. Occurances of normal integer values can also create trouble for data scientists to observe. So, its always a good practise to check for columns with string data type if they have integer values or special character. We can replace all such values with default values and again go to previous steps and run the process of cleaning data simultaneously.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['String_column_name'] = df['String_column_name'].str.replace('1','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('2','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('3','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('4','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('0','')\n",
    "#...\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('-','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('\"','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace(' ','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('(','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('.','')\n",
    "df['String_column_name'] = df['String_column_name'].str.replace('/','')\n",
    "#...\n",
    "#...\n",
    "#...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above process can be done initially or in later stages of data cleaning as per the need. String data processing can be tidious as well as conceptual when it comes to larger datasets. Many algorithms can also play a role in string cleaning within the datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Working on integers\n",
    "\n",
    "Many missing integer data sets can be replaced in several situations with the help of predictions. For example, if we have carbon datasets and we want to replace missing data based on year of emission given that we have trend of 40 years of data, we may give a value that is consistent with the trend based on some algorithm and replace missing data with the trend data. The same thing follows with the integer values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Output and save the cleaned dataframe to the require file format. \n",
    "\n",
    "This step can be done at several stages of the project depending on size of the data. Above mentioned steps can take longer time and its always a good idea to keep saving data in some file if processing takes longer time than usual and terminates in the middle (very less chances but happens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_data.csv', encoding = 'utf-8') # you may change file type depending on the requirements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font size=\"3.5\"> The above mentioned steps may or may not be applicable depending on the circumstances and the dataset. We assume all possible ways to clean data based on the datasets and start processing. Many other ways are there to clean data and several processing may further be added to the above mentioned steps before saving. You may try several other ways to come up with new ideas of data cleaning and make good decisions on ways to clean. We hope this blog will help you get started with data cleaning and exploring the ocean of Data Science. <p>  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " <B><p style=\"text-align:center; color: blue; font-size: 30px \"> Inncretech Project \n",
    " <br><br>\n",
    " <I style= \"text-align:center;color:black; font-size: 20px\"><B style = \"color:red\">Inn</B>ovation <B style = \"color:red\">Cre</B>ativity <B style = \"color:red\">Tech</B>nology: Engineering Imaginations</I>\n",
    " </B></p>\n",
    " \n",
    " <p>------------------------------------------------------------------------------------- END ---------------------------------------------------------------------------------------</p>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
